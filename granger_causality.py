# -*- coding: utf-8 -*-
"""Granger_Causality.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gY-05USjAYu2dAFaHJ6sDd17i3mgruJu

# **Causality Detection Based on Neural Network**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.regularizers import l2
from tensorflow.keras.callbacks import EarlyStopping
import yfinance as yf
from tensorflow.keras.optimizers import Adam

tickers = {
    'Stock': 'RELIANCE.NS',
    'Sensex': '^BSESN',
    'Nifty': '^NSEI',
    'Sectoral Index': '^CNXENERGY',
    'Gold': 'GC=F',
    'USD/INR': 'INR=X',
    'Oil': 'CL=F',
    'Interest Rate': '^IRX'
}
start_date = '2000-01-01'
end_date = '2023-01-01'
data = {}
for var, ticker in tickers.items():
    data[var] = yf.download(ticker, start=start_date, end=end_date)['Adj Close']
df = pd.DataFrame(data)

df.fillna(method='ffill', inplace=True)
df.dropna(inplace=True)

# Normalize the data
scaler = StandardScaler()
df_normalized = pd.DataFrame(scaler.fit_transform(df), columns=df.columns, index=df.index)

# Define the target variable
target_variable = 'Stock'
n_time_steps = 60
n_features = len(df.columns)

# Function to create lagged data for time series
def create_lagged_data(df, n_time_steps):
    X, y = [], []
    for i in range(n_time_steps, len(df)):
        X.append(df[i-n_time_steps:i].values)
        y.append(df[target_variable][i])
    return np.array(X), np.array(y)

# Train separate MLP models for each time series
influence_summary = {}
for col in df.columns:
    if col == target_variable:
        continue

    # Prepare data for MLP
    X, y = create_lagged_data(df_normalized, n_time_steps)

    # Split data
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Define the MLP Model
    model = Sequential()
    model.add(Dense(50, activation='relu', kernel_regularizer=l2(0.01), input_shape=(n_time_steps, n_features)))
    model.add(Dropout(0.2))
    model.add(Dense(1, kernel_regularizer=l2(0.01)))
    model.compile(optimizer=Adam(), loss='mse')
    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

    # Train the MLP Model
    history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, callbacks=[early_stopping])
    loss = model.evaluate(X_test, y_test)
    print(f'Test Loss for {col}: {loss}')

    # Analyze the MLP Weights for Granger Causality
    mlp_weights = model.layers[0].get_weights()
    input_weights = mlp_weights[0]  # Input weights have shape (input_dim, units)
    influence_summary[col] = np.sum(np.abs(input_weights), axis=1)

# Visualize Granger Causality Relationships
influence_df = pd.DataFrame.from_dict(influence_summary, orient='index', columns=df.columns)
influence_df.drop(columns=[target_variable], inplace=True)
influence_df = influence_df.sum(axis=1).sort_values(ascending=False)
print("Influence of Input Variables on Target Variable:")
print(influence_df)

plt.figure(figsize=(10, 6))
plt.bar(influence_df.index, influence_df.values)
plt.title('Influence of Input Variables on Target Variable')
plt.xlabel('Variables')
plt.ylabel('Influence')
plt.xticks(rotation=45)
plt.show()

# ===========================  LSTM CODE ===================================
n_time_steps = 60

# Reshape data for componentwise LSTM
X = []
for i in range(n_time_steps, len(df_normalized)):
    X.append(df_normalized[i-n_time_steps:i].values.T)  # Transpose for componentwise LSTM
X = np.array(X)

y = df_normalized[target_variable][n_time_steps:].values

# Step 3: Define the LSTM Model
lstm_models = []
for i in range(n_features):  # Create separate LSTM models for each input feature
    model = Sequential()
    model.add(LSTM(50, activation='relu', kernel_regularizer=l2(0.01), input_shape=(n_time_steps, 1)))
    model.add(Dropout(0.2))
    model.add(Dense(1, kernel_regularizer=l2(0.01)))
    model.compile(optimizer='adam', loss='mse')
    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
    lstm_models.append(model)

# Train the LSTM Model
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
lstm_histories = []
for i in range(n_features):
    history = lstm_models[i].fit(X_train[:, i, :], y_train, epochs=100, batch_size=32, validation_split=0.2, callbacks=[early_stopping])
    lstm_histories.append(history)
    loss = lstm_models[i].evaluate(X_test[:, i, :], y_test)
    print(f'Test Loss for Feature {i}: {loss}')

# Analyze the LSTM Weights for Granger Causality
lstm_weights = []
for i in range(n_features):
    lstm_weights.append(lstm_models[i].layers[0].get_weights()[0])  # Input weights for each feature

# Visualize Granger Causality Relationships
influence_summary = np.sum(np.abs(np.array(lstm_weights)), axis=(1, 2))  # Summing weights across all features and timesteps
influence_df = pd.DataFrame(influence_summary, index=df.columns, columns=['Influence'])
influence_df.sort_values(by='Influence', ascending=False, inplace=True)
print("Influence of Input Variables on Target Variable (LSTM):")
print(influence_df)

plt.figure(figsize=(10, 6))
plt.bar(influence_df.index, influence_df['Influence'])
plt.title('Influence of Input Variables on Target Variable (LSTM)')
plt.xlabel('Variables')
plt.ylabel('Influence')
plt.xticks(rotation=45)
plt.show()

"""# **Non Linear Causality - Wavelet Decomposition**"""

!pip install numpy pandas statsmodels PyWavelets yfinance

import numpy as np
import pandas as pd
import statsmodels.api as sm
import pywt
import matplotlib.pyplot as plt
from statsmodels.tsa.api import VAR
from statsmodels.tsa.stattools import adfuller, grangercausalitytests
import yfinance as yf

start = '2010-01-01'
end = '2020-01-01'
stock1 = yf.download('AAPL', start=start, end=end)['Adj Close']
stock2 = yf.download('MSFT', start=start, end=end)['Adj Close']
stock3 = yf.download('GOOGL', start=start, end=end)['Adj Close']
exchange_rate = yf.download('EURUSD=X', start=start, end=end)['Adj Close']

data = pd.concat([stock1, stock2, stock3, exchange_rate], axis=1)
data.columns = ['Stock1', 'Stock2', 'Stock3', 'ExchangeRate']
data = data.dropna()

# Transform into natural logarithms and first-difference
data['LogStock1'] = np.log(data['Stock1']).diff().dropna()
data['LogStock2'] = np.log(data['Stock2']).diff().dropna()
data['LogStock3'] = np.log(data['Stock3']).diff().dropna()
data['LogExchangeRate'] = np.log(data['ExchangeRate']).diff().dropna()

# Multiresolution Wavelet Decomposition
wavelet = 'sym4'
level = 3
coeffs_stock1 = pywt.wavedec(data['LogStock1'].dropna(), wavelet, level=level)
coeffs_stock2 = pywt.wavedec(data['LogStock2'].dropna(), wavelet, level=level)
coeffs_stock3 = pywt.wavedec(data['LogStock3'].dropna(), wavelet, level=level)
coeffs_exchange_rate = pywt.wavedec(data['LogExchangeRate'].dropna(), wavelet, level=level)

# Reconstruct detail coefficients (D1, D2, D3)
def reconstruct_details(coeffs, wavelet, level):
    details = []
    for i in range(1, level + 1):
        detail = pywt.upcoef('d', coeffs[i], wavelet, level=i, take=len(coeffs[0]))
        details.append(detail)
    return details

stock1_details = reconstruct_details(coeffs_stock1, wavelet, level)
stock2_details = reconstruct_details(coeffs_stock2, wavelet, level)
stock3_details = reconstruct_details(coeffs_stock3, wavelet, level)
exchange_rate_details = reconstruct_details(coeffs_exchange_rate, wavelet, level)

min_length = min(len(stock1_details[0]), len(stock2_details[0]), len(stock3_details[0]), len(exchange_rate_details[0]))
stock1_details = [detail[:min_length] for detail in stock1_details]
stock2_details = [detail[:min_length] for detail in stock2_details]
stock3_details = [detail[:min_length] for detail in stock3_details]
exchange_rate_details = [detail[:min_length] for detail in exchange_rate_details]

# Combine decomposed series into a DataFrame
decomposed_data = pd.DataFrame({
    'Stock1_D1': stock1_details[0],
    'Stock1_D2': stock1_details[1],
    'Stock1_D3': stock1_details[2],
    'Stock2_D1': stock2_details[0],
    'Stock2_D2': stock2_details[1],
    'Stock2_D3': stock2_details[2],
    'Stock3_D1': stock3_details[0],
    'Stock3_D2': stock3_details[1],
    'Stock3_D3': stock3_details[2],
    'ExchangeRate_D1': exchange_rate_details[0],
    'ExchangeRate_D2': exchange_rate_details[1],
    'ExchangeRate_D3': exchange_rate_details[2]
})

# Perform stationarity tests (ADF and PP)
def perform_stationarity_test(series, name):
    result = adfuller(series)
    print(f"ADF Test for {name}:")
    print(f"ADF Statistic: {result[0]:.4f}")
    print(f"p-value: {result[1]:.4f}")
    print(f"Number of Lags: {result[2]}")
    print(f"Number of Observations Used: {result[3]}")
    print(f"Critical Values:")
    for key, value in result[4].items():
        print(f"\t{key}: {value:.4f}")

perform_stationarity_test(decomposed_data['Stock1_D1'], 'Stock1_D1')
perform_stationarity_test(decomposed_data['Stock2_D1'], 'Stock2_D1')
# Perform tests for other decomposed series as needed

# VAR model estimation and Granger causality tests
def perform_granger_causality_test(data, var_order, cause_var, effect_var, kind='f'):
    model = VAR(data[[cause_var, effect_var]])
    results = model.fit(maxlags=var_order)
    causality_result = results.test_causality(cause_var, effect_var, kind=kind)
    print(f"Granger Causality Test ({cause_var} -> {effect_var}):")
    print(causality_result.summary())

var_order = 4  # Example order for VAR model
perform_granger_causality_test(
    decomposed_data, var_order, 'Stock1_D1', 'Stock2_D1'
)

# Visualization
decomposed_data.plot(subplots=True, figsize=(10, 8))
plt.show()

"""# **Another Example**"""

!pip install numpy pandas statsmodels PyWavelets yfinance fredapi

import numpy as np
import pandas as pd
import statsmodels.api as sm
import pywt
import matplotlib.pyplot as plt
from statsmodels.tsa.api import VAR
from statsmodels.tsa.stattools import adfuller, grangercausalitytests
import yfinance as yf
from fredapi import Fred

fred_api_key = '6a2911f2802549695d01c94eeb73aa14'

fred = Fred(api_key=fred_api_key)

start = '2010-01-01'
end = '2020-01-01'

gdp_growth = fred.get_series('GDP', start, end)
unemployment = fred.get_series('UNRATE', start, end)

exchange_rate = yf.download('INR=X', start=start, end=end)['Adj Close']

data = pd.concat([gdp_growth, unemployment, exchange_rate], axis=1)
data.columns = ['GDP_Growth', 'Unemployment', 'ExchangeRate']
data = data.dropna()

# Transform into natural logarithms and first-difference
data['LogGDP_Growth'] = np.log(data['GDP_Growth']).diff().dropna()
data['LogUnemployment'] = np.log(data['Unemployment']).diff().dropna()
data['LogExchangeRate'] = np.log(data['ExchangeRate']).diff().dropna()

# Multiresolution Wavelet Decomposition
wavelet = 'sym4'
level = 3
coeffs_gdp_growth = pywt.wavedec(data['LogGDP_Growth'].dropna(), wavelet, level=level)
coeffs_unemployment = pywt.wavedec(data['LogUnemployment'].dropna(), wavelet, level=level)
coeffs_exchange_rate = pywt.wavedec(data['LogExchangeRate'].dropna(), wavelet, level=level)

# Reconstruct detail coefficients (D1, D2, D3)
def reconstruct_details(coeffs, wavelet, level):
    details = []
    for i in range(1, level + 1):
        detail = pywt.upcoef('d', coeffs[i], wavelet, level=i, take=len(coeffs[0]))
        details.append(detail)
    return details

gdp_growth_details = reconstruct_details(coeffs_gdp_growth, wavelet, level)
unemployment_details = reconstruct_details(coeffs_unemployment, wavelet, level)
exchange_rate_details = reconstruct_details(coeffs_exchange_rate, wavelet, level)

min_length = min(len(gdp_growth_details[0]), len(unemployment_details[0]), len(exchange_rate_details[0]))
gdp_growth_details = [detail[:min_length] for detail in gdp_growth_details]
unemployment_details = [detail[:min_length] for detail in unemployment_details]
exchange_rate_details = [detail[:min_length] for detail in exchange_rate_details]

# Combine decomposed series into a DataFrame
decomposed_data = pd.DataFrame({
    'GDP_Growth_D1': gdp_growth_details[0],
    'GDP_Growth_D2': gdp_growth_details[1],
    'GDP_Growth_D3': gdp_growth_details[2],
    'Unemployment_D1': unemployment_details[0],
    'Unemployment_D2': unemployment_details[1],
    'Unemployment_D3': unemployment_details[2],
    'ExchangeRate_D1': exchange_rate_details[0],
    'ExchangeRate_D2': exchange_rate_details[1],
    'ExchangeRate_D3': exchange_rate_details[2]
})

# Perform stationarity tests (ADF and PP)
def perform_stationarity_test(series, name):
    result = adfuller(series)
    print(f"ADF Test for {name}:")
    print(f"ADF Statistic: {result[0]:.4f}")
    print(f"p-value: {result[1]:.4f}")
    print(f"Number of Lags: {result[2]}")
    print(f"Number of Observations Used: {result[3]}")
    print(f"Critical Values:")
    for key, value in result[4].items():
        print(f"\t{key}: {value:.4f}")

perform_stationarity_test(decomposed_data['GDP_Growth_D1'], 'GDP_Growth_D1')
perform_stationarity_test(decomposed_data['Unemployment_D1'], 'Unemployment_D1')

# VAR model estimation and Granger causality tests
def perform_granger_causality_test(data, var_order, cause_var, effect_var, kind='f'):
    model = VAR(data[[cause_var, effect_var]])
    results = model.fit(maxlags=var_order)
    causality_result = results.test_causality(cause_var, effect_var, kind=kind)
    print(f"Granger Causality Test ({cause_var} -> {effect_var}):")
    print(causality_result.summary())

# Adjust var_order based on your data and analysis
var_order = 4  # Example order for VAR model
perform_granger_causality_test(
    decomposed_data, var_order, 'GDP_Growth_D1', 'Unemployment_D1'
)

# Visualization
decomposed_data.plot(subplots=True, figsize=(10, 8))
plt.show()

!pip install numpy pandas statsmodels PyWavelets yfinance fredapi scikit-learn

import numpy as np
import pandas as pd
import statsmodels.api as sm
import pywt
import matplotlib.pyplot as plt
from statsmodels.tsa.api import VAR
from statsmodels.tsa.stattools import adfuller, grangercausalitytests
import yfinance as yf
from fredapi import Fred
from sklearn.neighbors import KernelDensity
from scipy.stats import norm

fred_api_key = '6a2911f2802549695d01c94eeb73aa14'

fred = Fred(api_key=fred_api_key)

start = '2010-01-01'
end = '2020-01-01'

gdp_growth = fred.get_series('A191RL1Q225SBEA', start, end)  # Real GDP per capita growth rate
cpi = fred.get_series('CPIAUCSL', start, end)  # Consumer Price Index for All Urban Consumers: All Items in U.S. City Average
exchange_rate = yf.download('INR=X', start=start, end=end)['Adj Close']

data = pd.concat([gdp_growth, cpi, exchange_rate], axis=1)
data.columns = ['GDP_Growth', 'CPI', 'ExchangeRate']
data = data.dropna()

# Transform into natural logarithms and first-difference
data['LogGDP_Growth'] = np.log(data['GDP_Growth']).diff().dropna()
data['LogCPI'] = np.log(data['CPI']).diff().dropna()
data['LogExchangeRate'] = np.log(data['ExchangeRate']).diff().dropna()

# Multiresolution Wavelet Decomposition
wavelet = 'sym4'
level = 3
coeffs_gdp_growth = pywt.wavedec(data['LogGDP_Growth'].dropna(), wavelet, level=level)
coeffs_cpi = pywt.wavedec(data['LogCPI'].dropna(), wavelet, level=level)
coeffs_exchange_rate = pywt.wavedec(data['LogExchangeRate'].dropna(), wavelet, level=level)

# Reconstruct detail coefficients (D1, D2, D3)
def reconstruct_details(coeffs, wavelet, level):
    details = []
    for i in range(1, level + 1):
        detail = pywt.upcoef('d', coeffs[i], wavelet, level=i, take=len(coeffs[0]))
        details.append(detail)
    return details

gdp_growth_details = reconstruct_details(coeffs_gdp_growth, wavelet, level)
cpi_details = reconstruct_details(coeffs_cpi, wavelet, level)
exchange_rate_details = reconstruct_details(coeffs_exchange_rate, wavelet, level)

# Ensure all series are of the same length
min_length = min(len(gdp_growth_details[0]), len(cpi_details[0]), len(exchange_rate_details[0]))
gdp_growth_details = [detail[:min_length] for detail in gdp_growth_details]
cpi_details = [detail[:min_length] for detail in cpi_details]
exchange_rate_details = [detail[:min_length] for detail in exchange_rate_details]

# Combine decomposed series into a DataFrame
decomposed_data = pd.DataFrame({
    'GDP_Growth_D1': gdp_growth_details[0],
    'GDP_Growth_D2': gdp_growth_details[1],
    'GDP_Growth_D3': gdp_growth_details[2],
    'CPI_D1': cpi_details[0],
    'CPI_D2': cpi_details[1],
    'CPI_D3': cpi_details[2],
    'ExchangeRate_D1': exchange_rate_details[0],
    'ExchangeRate_D2': exchange_rate_details[1],
    'ExchangeRate_D3': exchange_rate_details[2]
})

# Perform stationarity tests (ADF and PP)
def perform_stationarity_test(series, name):
    result = adfuller(series)
    print(f"ADF Test for {name}:")
    print(f"ADF Statistic: {result[0]:.4f}")
    print(f"p-value: {result[1]:.4f}")
    print(f"Number of Lags: {result[2]}")
    print(f"Number of Observations Used: {result[3]}")
    print(f"Critical Values:")
    for key, value in result[4].items():
        print(f"\t{key}: {value:.4f}")

perform_stationarity_test(decomposed_data['GDP_Growth_D1'], 'GDP_Growth_D1')
perform_stationarity_test(decomposed_data['CPI_D1'], 'CPI_D1')
# Perform tests for other decomposed series as needed

# VAR model estimation and Granger causality tests
def perform_granger_causality_test(data, var_order, cause_var, effect_var, kind='f'):
    model = VAR(data[[cause_var, effect_var]])
    results = model.fit(maxlags=var_order)
    causality_result = results.test_causality(cause_var, effect_var, kind=kind)
    print(f"Granger Causality Test ({cause_var} -> {effect_var}):")
    print(causality_result.summary())

# Adjust var_order based on your data and analysis
var_order = 4
perform_granger_causality_test(
    decomposed_data, var_order, 'GDP_Growth_D1', 'CPI_D1'
)
# Perform tests for other frequency bands and variable pairs as needed

# Visualization
decomposed_data.plot(subplots=True, figsize=(10, 8))
plt.show()

def dp_test(X, Y, epsilon=1.5, bandwidth=1):
    def local_density(X, bandwidth):
        kde = KernelDensity(kernel='gaussian', bandwidth=bandwidth).fit(X)
        log_density = kde.score_samples(X)
        return np.exp(log_density)

    # Ensure consistent length for X and Y
    min_length = min(len(X), len(Y))
    X = np.array(X[:min_length]).reshape(-1, 1)
    Y = np.array(Y[:min_length]).reshape(-1, 1)

    # Joint density estimation
    XY = np.hstack((X, Y))
    f_XY = local_density(XY, bandwidth)

    # Marginal density estimations
    f_X = local_density(X, bandwidth)
    f_Y = local_density(Y, bandwidth)

    dp_stat = np.mean(f_XY * f_Y - f_X * f_Y)
    dp_stat /= np.std(f_XY * f_Y - f_X * f_Y) / np.sqrt(len(X))

    return dp_stat, norm.cdf(dp_stat)

dp_stat, p_value = dp_test(decomposed_data['GDP_Growth_D1'], decomposed_data['CPI_D1'])
print(f"DP Test Statistic: {dp_stat}")
print(f"P-Value: {p_value}")



"""Permutation - Granger Causality"""

import pandas as pd
import numpy as np
import yfinance as yf
from sklearn.model_selection import train_test_split

def download_data(tickers, start_date, end_date):
    data = yf.download(tickers, start=start_date, end=end_date)
    return data['Adj Close']

tickers = ['^GSPC', '^IXIC']
data = download_data(tickers, '2010-01-01', '2023-01-01')

data = data.dropna()

train_data, test_data = train_test_split(data, test_size=0.2, shuffle=False)

train_mean = train_data.mean()
train_std = train_data.std()
train_data = (train_data - train_mean) / train_std
test_data = (test_data - train_mean) / train_std

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

class FeedForwardNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(FeedForwardNN, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        out = self.fc1(x)
        out = self.relu(out)
        out = self.fc2(out)
        return out

def prepare_data(data, lag):
    X, y = [], []
    for i in range(len(data) - lag):
        X.append(data.iloc[i:i+lag].values.flatten())
        y.append(data.iloc[i+lag].values)
    return np.array(X), np.array(y)

lag = 5
X_train, y_train = prepare_data(train_data, lag)
X_test, y_test = prepare_data(test_data, lag)

train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))
test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32))

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

# Define model, loss function, and optimizer
input_size = X_train.shape[1]
hidden_size = 50
output_size = y_train.shape[1]
model = FeedForwardNN(input_size, hidden_size, output_size)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training the model
def train_model(model, train_loader, criterion, optimizer, epochs):
    model.train()
    for epoch in range(epochs):
        for X_batch, y_batch in train_loader:
            optimizer.zero_grad()
            outputs = model(X_batch)
            loss = criterion(outputs, y_batch)
            loss.backward()
            optimizer.step()
        print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}')

train_model(model, train_loader, criterion, optimizer, epochs=50)

def permute_data(data):
    permuted_data = data.copy()
    np.random.shuffle(permuted_data)
    return permuted_data

# Create multiple permutations
num_permutations = 100
permuted_data_sets = [permute_data(X_train) for _ in range(num_permutations)]

# Evaluate variance of prediction errors for original and permuted data sets
def evaluate_model(model, data_loader, criterion):
    model.eval()
    losses = []
    with torch.no_grad():
        for X_batch, y_batch in data_loader:
            outputs = model(X_batch)
            loss = criterion(outputs, y_batch)
            losses.append(loss.item())
    return np.mean(losses)

original_error = evaluate_model(model, test_loader, criterion)
permuted_errors = [evaluate_model(model, DataLoader(TensorDataset(torch.tensor(data, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32)), batch_size=32, shuffle=False), criterion) for data in permuted_data_sets]

# Calculate p-value
p_value = np.mean([error < original_error for error in permuted_errors])
print(f'p-value: {p_value}')

