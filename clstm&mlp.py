# -*- coding: utf-8 -*-
"""cLSTM&MLP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Lyw2smVc3Q-cErR9I7aO2qTwJDocW7uG
"""

# Install necessary packages in Colab
!pip install tensorflow pandas numpy openpyxl scikit-learn

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.layers import Input, LSTM, Dense, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.regularizers import Regularizer
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# -------------------------------
# Custom Group Lasso Regularizer
# -------------------------------
class GroupLassoRegularizer(Regularizer):
    def __init__(self, lmbda):
        self.lmbda = lmbda

    def __call__(self, weight_matrix):
        # Compute l2 norm for each group (i.e. each row)
        group_norms = tf.norm(weight_matrix, axis=1)
        return self.lmbda * tf.reduce_sum(group_norms)

    def get_config(self):
        return {'lmbda': self.lmbda}

# -------------------------------
# Data Loading and Preprocessing
# -------------------------------
# Load data from the Excel file (adjust file path if necessary)
df_crypto = pd.read_excel('/content/combined_data.xlsx', sheet_name='AllData')

# Select and rename columns for BTC, ETH, ADA, XRP, BCH, XAU, and CrudeOil
df_closing = df_crypto[['Date', 'BTC_Close', 'ETH_Close', 'ADA_Close',
                         'XRP_Close', 'BCH_Close', 'XAU_Close', 'CrudeOil_Close']]
df_closing.columns = ['Date', 'BTC_Close', 'ETH_Close', 'ADA_Close',
                        'XRP_Close', 'BCH_Close', 'XAU_Close', 'CrudeOil_Close']
df_closing['Date'] = pd.to_datetime(df_closing['Date'])
df_closing.dropna(inplace=True)
df_closing.sort_values('Date', inplace=True)

# Define the asset columns
crypto_columns = ['BTC_Close', 'ETH_Close', 'ADA_Close', 'XRP_Close',
                  'BCH_Close', 'XAU_Close', 'CrudeOil_Close']

# Ensure stationarity: take the first difference
df_stationary = df_closing.copy()
for col in crypto_columns:
    df_stationary[col] = df_stationary[col].diff()
df_stationary.dropna(inplace=True)

# Normalize features (important for NN training)
scaler = StandardScaler()
df_stationary[crypto_columns] = scaler.fit_transform(df_stationary[crypto_columns])

# -------------------------------
# Neural Network Granger Causality Analysis
# -------------------------------
def analyze_granger(df, target, features, timesteps=10, units=20, lmbda=0.01, epochs=50, batch_size=16):
    X = df[features].values
    y = df[target].values

    # Create sequential data
    X_seq, y_seq = [], []
    for i in range(timesteps, len(X)):
        X_seq.append(X[i-timesteps:i, :])
        y_seq.append(y[i])
    X_seq = np.array(X_seq, dtype='float32')
    y_seq = np.array(y_seq, dtype='float32')

    # Split data into training and validation sets (no shuffling for time series)
    X_train, X_val, y_train, y_val = train_test_split(X_seq, y_seq, test_size=0.2, shuffle=False)

    # Build the model: LSTM with Group Lasso regularization and additional dropout
    input_seq = Input(shape=(timesteps, len(features)))
    lstm_out = LSTM(units, kernel_regularizer=GroupLassoRegularizer(lmbda),
                    return_sequences=False)(input_seq)
    dropout_out = Dropout(0.2)(lstm_out)
    output = Dense(1)(dropout_out)

    model = Model(inputs=input_seq, outputs=output)
    model.compile(optimizer='adam', loss='mse')

    # Callbacks for early stopping and adaptive learning rate reduction
    callbacks = [
        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),
        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)
    ]
    model.fit(X_train, y_train, validation_data=(X_val, y_val),
              epochs=epochs, batch_size=batch_size, callbacks=callbacks, verbose=1)

    # Extract the LSTM cell kernel weights and compute group norms for feature importance
    kernel_weights = model.layers[1].cell.kernel.numpy()
    group_norms = np.linalg.norm(kernel_weights, axis=1)

    return dict(zip(features, group_norms))

# Run iterative analysis: treat each asset as target, the others as features.
results = {}
for target in crypto_columns:
    feature_cols = [col for col in crypto_columns if col != target]
    print(f"\nAnalyzing target: {target}")
    norms = analyze_granger(df_stationary, target, feature_cols, epochs=50)
    results[target] = norms

# Convert results (a dictionary of dictionaries) into a square DataFrame with NaN on the diagonal.
desired_order = ['BTC_Close', 'ETH_Close', 'ADA_Close', 'XRP_Close', 'BCH_Close', 'XAU_Close', 'CrudeOil_Close']
group_norm_matrix = pd.DataFrame(index=desired_order, columns=desired_order)

for target in desired_order:
    for feature in desired_order:
        if target == feature:
            group_norm_matrix.loc[target, feature] = np.nan
        else:
            # results[target] contains group norms for features used when target is predicted.
            group_norm_matrix.loc[target, feature] = results[target].get(feature, np.nan)

print("Granger causality (group norm) analysis results (rearranged):")
print(group_norm_matrix)

# -------------------------------
# Compute and Reorder Pearson Correlation Matrix (unchanged)
# -------------------------------
corr_matrix = df_stationary[crypto_columns].corr()
corr_matrix = corr_matrix.reindex(index=desired_order, columns=desired_order)
print("Reordered Pearson correlation matrix:")
print(corr_matrix)

# Install necessary visualization packages
!pip install seaborn matplotlib

import seaborn as sns
import matplotlib.pyplot as plt

# Set Times New Roman font and other style parameters
plt.rcParams['font.family'] = 'Times New Roman'
plt.rcParams['font.size'] = 14
sns.set_style("whitegrid")
sns.set_context("talk", font_scale=1)

# -------------------------------
# Prepare display DataFrames without "_Close"
# (Assuming group_norm_matrix and corr_matrix are defined above)
# -------------------------------
df_results_display = group_norm_matrix.copy()
df_results_display.index = df_results_display.index.str.replace('_Close', '')
df_results_display.columns = df_results_display.columns.str.replace('_Close', '')

# Force numeric (coercing any problematic cells to NaN)
df_results_display = df_results_display.apply(pd.to_numeric, errors='coerce')

corr_matrix_display = corr_matrix.copy()
corr_matrix_display.index = corr_matrix_display.index.str.replace('_Close', '')
corr_matrix_display.columns = corr_matrix_display.columns.str.replace('_Close', '')

# Also ensure correlation matrix is numeric if needed
corr_matrix_display = corr_matrix_display.apply(pd.to_numeric, errors='coerce')

# -------------------------------
# Visualize Granger Causality (Group Norms) Heatmap
# -------------------------------
plt.figure(figsize=(10, 8))
sns.heatmap(
    df_results_display,
    annot=True,
    fmt=".2f",
    cmap='coolwarm',
    linewidths=0.5,
    square=True,
    cbar_kws={'shrink': 0.8}
)
plt.title('Granger Causality (Group Norms) Heatmap', fontsize=16, fontweight='bold')
plt.xlabel('Features (Predictors)', fontsize=14)
plt.ylabel('Target Variables', fontsize=14)
plt.tight_layout()
plt.show()

# -------------------------------
# Visualize Pearson Correlation Matrix Heatmap
# -------------------------------
plt.figure(figsize=(10, 8))
sns.heatmap(
    corr_matrix_display,
    annot=True,
    fmt=".2f",
    cmap='coolwarm',
    linewidths=0.5,
    square=True,
    cbar_kws={'shrink': 0.8}
)
plt.title('Pearson Correlation Matrix', fontsize=16, fontweight='bold')
plt.xlabel('Variables', fontsize=14)
plt.ylabel('Variables', fontsize=14)
plt.tight_layout()
plt.show()

# Install necessary packages in Colab
!pip install tensorflow pandas numpy openpyxl scikit-learn seaborn matplotlib

import tensorflow as tf
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from tensorflow.keras.layers import Input, LSTM, Dense, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.regularizers import Regularizer
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# Set visualization style for publication-quality graphics
plt.rcParams['font.family'] = 'Times New Roman'
plt.rcParams['font.size'] = 14
sns.set_style("whitegrid")
sns.set_context("talk", font_scale=1)

# -------------------------------
# Custom Group Lasso Regularizer for LSTM
# -------------------------------
class GroupLassoRegularizer(Regularizer):
    def __init__(self, lmbda):
        self.lmbda = lmbda

    def __call__(self, weight_matrix):
        # Compute L2 norm for each row (group)
        group_norms = tf.norm(weight_matrix, axis=1)
        return self.lmbda * tf.reduce_sum(group_norms)

    def get_config(self):
        return {'lmbda': self.lmbda}

# -------------------------------
# Data Loading and Preprocessing
# -------------------------------
# Load data from the Excel file (update path if necessary)
df_crypto = pd.read_excel('/content/combined_data.xlsx', sheet_name='AllData')

# Select and rename columns for BTC, ETH, ADA, XRP, BCH, XAU, and CrudeOil
df_closing = df_crypto[['Date', 'BTC_Close', 'ETH_Close', 'ADA_Close',
                         'XRP_Close', 'BCH_Close', 'XAU_Close', 'CrudeOil_Close']]
df_closing.columns = ['Date', 'BTC_Close', 'ETH_Close', 'ADA_Close',
                        'XRP_Close', 'BCH_Close', 'XAU_Close', 'CrudeOil_Close']
df_closing['Date'] = pd.to_datetime(df_closing['Date'])
df_closing.dropna(inplace=True)
df_closing.sort_values('Date', inplace=True)

# Define the asset columns
crypto_columns = ['BTC_Close', 'ETH_Close', 'ADA_Close', 'XRP_Close', 'BCH_Close', 'XAU_Close', 'CrudeOil_Close']

# Ensure stationarity: take the first difference
df_stationary = df_closing.copy()
for col in crypto_columns:
    df_stationary[col] = df_stationary[col].diff()
df_stationary.dropna(inplace=True)

# Normalize features (important for NN training)
scaler = StandardScaler()
df_stationary[crypto_columns] = scaler.fit_transform(df_stationary[crypto_columns])

# -------------------------------
# Neural Network Granger Causality Analysis Function
# -------------------------------
def analyze_granger(df, target, features, timesteps=10, units=20, lmbda=0.01, epochs=50, batch_size=16):
    X = df[features].values
    y = df[target].values

    # Create sequential data
    X_seq, y_seq = [], []
    for i in range(timesteps, len(X)):
        X_seq.append(X[i-timesteps:i, :])
        y_seq.append(y[i])
    X_seq = np.array(X_seq, dtype='float32')
    y_seq = np.array(y_seq, dtype='float32')

    # Split data into training and validation sets (no shuffling for time series)
    X_train, X_val, y_train, y_val = train_test_split(X_seq, y_seq, test_size=0.2, shuffle=False)

    # Build the model: LSTM with Group Lasso regularization and additional dropout
    input_seq = Input(shape=(timesteps, len(features)))
    lstm_out = LSTM(units, kernel_regularizer=GroupLassoRegularizer(lmbda),
                    return_sequences=False)(input_seq)
    dropout_out = Dropout(0.2)(lstm_out)
    output = Dense(1)(dropout_out)
    model = Model(inputs=input_seq, outputs=output)
    model.compile(optimizer='adam', loss='mse')

    # Callbacks for early stopping and adaptive learning rate reduction
    callbacks = [
        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),
        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)
    ]
    model.fit(X_train, y_train, validation_data=(X_val, y_val),
              epochs=epochs, batch_size=batch_size, callbacks=callbacks, verbose=1)

    # Extract the LSTM cell kernel weights and compute group norms for feature importance
    kernel_weights = model.layers[1].cell.kernel.numpy()
    group_norms = np.linalg.norm(kernel_weights, axis=1)

    return dict(zip(features, group_norms))

# -------------------------------
# Run Iterative Granger Causality Analysis
# -------------------------------
results = {}
for target in crypto_columns:
    feature_cols = [col for col in crypto_columns if col != target]
    print(f"\nAnalyzing target: {target}")
    norms = analyze_granger(df_stationary, target, feature_cols, timesteps=10, units=20, lmbda=0.01, epochs=50, batch_size=16)
    results[target] = norms

# Convert results (dictionary of dictionaries) into a square DataFrame with NaN on the diagonal.
desired_order = ['BTC_Close', 'ETH_Close', 'ADA_Close', 'XRP_Close', 'BCH_Close', 'XAU_Close', 'CrudeOil_Close']
group_norm_matrix = pd.DataFrame(index=desired_order, columns=desired_order)
for target in desired_order:
    for feature in desired_order:
        if target == feature:
            group_norm_matrix.loc[target, feature] = np.nan
        else:
            group_norm_matrix.loc[target, feature] = results[target].get(feature, np.nan)

print("Granger causality (group norm) analysis results (rearranged):")
print(group_norm_matrix)

# -------------------------------
# Compute and Reorder Pearson Correlation Matrix (for reference)
# -------------------------------
corr_matrix = df_stationary[crypto_columns].corr()
corr_matrix = corr_matrix.reindex(index=desired_order, columns=desired_order)
print("Reordered Pearson correlation matrix:")
print(corr_matrix)

# -------------------------------
# Visualization: Heatmaps for Group Norms and Pearson Correlation
# -------------------------------
# Function to clean labels (remove '_Close' suffix)
def clean_labels(df):
    df_clean = df.copy()
    df_clean.index = df_clean.index.str.replace('_Close', '')
    df_clean.columns = df_clean.columns.str.replace('_Close', '')
    return df_clean

df_results_display = clean_labels(group_norm_matrix).astype(float)
corr_matrix_display = clean_labels(corr_matrix).astype(float)

# Heatmap for Granger causality (group norms)
plt.figure(figsize=(10, 8))
sns.heatmap(df_results_display, annot=True, fmt=".2f", cmap='coolwarm', linewidths=0.5, square=True, cbar_kws={'shrink': 0.8})
plt.title('Granger Causality (Group Norms) Heatmap', fontsize=16, fontweight='bold')
plt.xlabel('Features (Predictors)', fontsize=14)
plt.ylabel('Target Variables', fontsize=14)
plt.tight_layout()
plt.show()

# Heatmap for Pearson correlation matrix
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix_display, annot=True, fmt=".2f", cmap='coolwarm', linewidths=0.5, square=True, cbar_kws={'shrink': 0.8})
plt.title('Pearson Correlation Matrix', fontsize=16, fontweight='bold')
plt.xlabel('Variables', fontsize=14)
plt.ylabel('Variables', fontsize=14)
plt.tight_layout()
plt.show()

import tensorflow as tf
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from tensorflow.keras.layers import Input, LSTM, Dense, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.regularizers import Regularizer
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from scipy.stats import percentileofscore
from joblib import Parallel, delayed

# -------------------------------
# Custom Group Lasso Regularizer for LSTM
# -------------------------------
class GroupLassoRegularizer(Regularizer):
    def __init__(self, lmbda):
        self.lmbda = lmbda

    def __call__(self, weight_matrix):
        group_norms = tf.norm(weight_matrix, axis=1)
        return self.lmbda * tf.reduce_sum(group_norms)

    def get_config(self):
        return {'lmbda': self.lmbda}

# -------------------------------
# Neural Network Granger Causality Analysis Function
# -------------------------------
def analyze_granger(df, target, features, timesteps=10, units=10, lmbda=0.01, epochs=30, batch_size=16):
    X = df[features].values
    y = df[target].values

    # Create sequential data
    X_seq, y_seq = [], []
    for i in range(timesteps, len(X)):
        X_seq.append(X[i - timesteps:i, :])
        y_seq.append(y[i])
    X_seq = np.array(X_seq, dtype='float32')
    y_seq = np.array(y_seq, dtype='float32')

    # Split data into training and validation sets
    X_train, X_val, y_train, y_val = train_test_split(X_seq, y_seq, test_size=0.2, shuffle=False)

    # Build the LSTM model with Group Lasso
    input_seq = Input(shape=(timesteps, len(features)))
    lstm_out = LSTM(units, kernel_regularizer=GroupLassoRegularizer(lmbda), return_sequences=False)(input_seq)
    dropout_out = Dropout(0.2)(lstm_out)
    output = Dense(1)(dropout_out)
    model = Model(inputs=input_seq, outputs=output)
    model.compile(optimizer='adam', loss='mse')

    # Callbacks
    callbacks = [
        EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),
        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)
    ]

    # Train the model
    model.fit(X_train, y_train, validation_data=(X_val, y_val),
              epochs=epochs, batch_size=batch_size, callbacks=callbacks, verbose=0)

    # Extract weights and compute group norms
    kernel_weights = model.layers[1].cell.kernel.numpy()
    group_norms = np.linalg.norm(kernel_weights, axis=1)

    return dict(zip(features, group_norms))

# -------------------------------
# Optimized Permutation Testing (Parallelized)
# -------------------------------
def parallel_permutation_test(df, target, features, timesteps=10, units=10, lmbda=0.01, epochs=30, batch_size=16, num_permutations=30):
    null_distributions = {feature: [] for feature in features}

    # Train model once on real data
    actual_norms = analyze_granger(df, target, features, timesteps, units, lmbda, epochs, batch_size)

    def shuffle_and_train(_):
        df_shuffled = df.copy()
        np.random.shuffle(df_shuffled[target].values)  # Shuffle only the target variable

        shuffled_norms = analyze_granger(df_shuffled, target, features, timesteps, units, lmbda, epochs, batch_size)
        return shuffled_norms

    # Run permutations in parallel
    results = Parallel(n_jobs=-1)(delayed(shuffle_and_train)(i) for i in range(num_permutations))

    # Store results
    for result in results:
        for feature in features:
            null_distributions[feature].append(result[feature])

    return actual_norms, null_distributions

# -------------------------------
# Statistical Significance Testing
# -------------------------------
def compute_p_values(actual_norms, null_distributions):
    p_values = {}
    for feature in actual_norms:
        percentile = percentileofscore(null_distributions[feature], actual_norms[feature])
        p_value = 1 - (percentile / 100)  # Convert to right-tailed test
        p_values[feature] = p_value
    return p_values

# -------------------------------
# Load & Preprocess Data
# -------------------------------
df_crypto = pd.read_excel('/content/combined_data.xlsx', sheet_name='AllData')

df_closing = df_crypto[['Date', 'BTC_Close', 'ETH_Close', 'ADA_Close',
                        'XRP_Close', 'BCH_Close', 'XAU_Close', 'CrudeOil_Close']]
df_closing.columns = ['Date', 'BTC_Close', 'ETH_Close', 'ADA_Close',
                      'XRP_Close', 'BCH_Close', 'XAU_Close', 'CrudeOil_Close']
df_closing['Date'] = pd.to_datetime(df_closing['Date'])
df_closing.dropna(inplace=True)
df_closing.sort_values('Date', inplace=True)

crypto_columns = ['BTC_Close', 'ETH_Close', 'ADA_Close', 'XRP_Close', 'BCH_Close', 'XAU_Close', 'CrudeOil_Close']

# Ensure stationarity: take the first difference
df_stationary = df_closing.copy()
for col in crypto_columns:
    df_stationary[col] = df_stationary[col].diff()
df_stationary.dropna(inplace=True)

# Normalize features
scaler = StandardScaler()
df_stationary[crypto_columns] = scaler.fit_transform(df_stationary[crypto_columns])

# -------------------------------
# Run Statistical Granger Causality Analysis
# -------------------------------
num_permutations = 30  # Reduced for efficiency
results = {}
null_distributions_all = {}
p_values_all = {}

for target in crypto_columns:
    feature_cols = [col for col in crypto_columns if col != target]
    print(f"\nAnalyzing target: {target}")

    # Compute actual group norms and null distributions using optimized permutation test
    actual_norms, null_distributions = parallel_permutation_test(df_stationary, target, feature_cols, timesteps=10, units=10, lmbda=0.01, epochs=30, batch_size=16, num_permutations=num_permutations)

    # Compute p-values
    p_values = compute_p_values(actual_norms, null_distributions)

    # Store results
    results[target] = actual_norms
    null_distributions_all[target] = null_distributions
    p_values_all[target] = p_values

# -------------------------------
# Convert Results into DataFrame
# -------------------------------
p_value_matrix = pd.DataFrame(index=crypto_columns, columns=crypto_columns)
for target in crypto_columns:
    for feature in crypto_columns:
        if target == feature:
            p_value_matrix.loc[target, feature] = np.nan
        else:
            p_value_matrix.loc[target, feature] = p_values_all[target].get(feature, np.nan)

print("\nP-values for Granger Causality (Permutation Testing):")
print(p_value_matrix)

# -------------------------------
# Heatmap of P-values
# -------------------------------
plt.figure(figsize=(10, 8))
sns.heatmap(p_value_matrix.astype(float), annot=True, fmt=".4f", cmap='coolwarm', linewidths=0.5, square=True, cbar_kws={'shrink': 0.8})
plt.title('Granger Causality P-values (Permutation Test)', fontsize=16, fontweight='bold')
plt.xlabel('Predictor Variables', fontsize=14)
plt.ylabel('Target Variables', fontsize=14)
plt.tight_layout()
plt.show()

# -------------------------------
# Import necessary libraries
# -------------------------------
import pandas as pd
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
import seaborn as sns
from joblib import Parallel, delayed
from tensorflow.keras.layers import Input, LSTM, Dense
from tensorflow.keras.models import Model
from tensorflow.keras.regularizers import Regularizer
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from scipy.stats import percentileofscore

# -------------------------------
# USER-DEFINED PARAMETERS
# -------------------------------
# High computation mode adjusts hyperparameters to more demanding settings.
HIGH_COMPUTATION_MODE = False
num_permutations = 30 if not HIGH_COMPUTATION_MODE else 100  # Number of permutation tests
num_jobs = 4 if not HIGH_COMPUTATION_MODE else -1              # Parallel jobs (-1 uses all processors)
units = 10 if not HIGH_COMPUTATION_MODE else 20                # Number of LSTM units
epochs = 20 if not HIGH_COMPUTATION_MODE else 50               # Number of training epochs
batch_size = 32 if not HIGH_COMPUTATION_MODE else 16           # Training batch size

timesteps = 10  # Use 10 timesteps for LSTM input sequences (explicit time lag)
lmbda = 0.01    # Regularization strength for Group Lasso

# -------------------------------
# Data Loading and Preprocessing
# -------------------------------
# Load the dataset from Excel (update file path and sheet name if needed)
df_crypto = pd.read_excel('/content/combined_data.xlsx', sheet_name='AllData')

# Define the required columns that must exist in the data file.
required_cols = ['Date', 'BTC_Close', 'ETH_Close', 'ADA_Close',
                 'XRP_Close', 'BCH_Close', 'XAU_Close', 'CrudeOil_Close']

# Verify that all required columns exist; if not, raise an error.
missing_cols = [col for col in required_cols if col not in df_crypto.columns]
if missing_cols:
    raise KeyError(f"Missing columns in input file: {missing_cols}. Please ensure the file has the correct column names.")

# Select only the required columns and create a copy.
df_closing = df_crypto[required_cols].copy()

# Convert 'Date' column to datetime, drop missing values, and sort by date.
df_closing.loc[:, 'Date'] = pd.to_datetime(df_closing['Date'])
df_closing.dropna(inplace=True)
df_closing.sort_values('Date', inplace=True)

# Define the list of asset closing price columns (excluding 'Date').
crypto_columns = ['BTC_Close', 'ETH_Close', 'ADA_Close', 'XRP_Close',
                  'BCH_Close', 'XAU_Close', 'CrudeOil_Close']

# Ensure stationarity by taking the first difference of each series.
df_stationary = df_closing.copy()
for col in crypto_columns:
    df_stationary.loc[:, col] = df_stationary[col].diff()
df_stationary.dropna(inplace=True)

# Normalize the features using StandardScaler.
scaler = StandardScaler()
df_stationary[crypto_columns] = scaler.fit_transform(df_stationary[crypto_columns])

# -------------------------------
# Custom Group Lasso Regularizer
# -------------------------------
class GroupLassoRegularizer(Regularizer):
    """
    Custom regularizer that applies group Lasso on the LSTM kernel weights.
    It reshapes the kernel so that weights corresponding to each input feature (across all LSTM gates)
    are grouped together, and then computes their L2 norm.
    """
    def __init__(self, lmbda, n_features):
        self.lmbda = lmbda            # Regularization strength
        self.n_features = n_features  # Number of input features

    def __call__(self, weight_matrix):
        # LSTM kernel shape is (input_dim, 4 * units). Reshape to (n_features, -1)
        weight_2d = tf.reshape(weight_matrix, (self.n_features, -1))
        # Compute the L2 norm for each feature group.
        group_norms = tf.norm(weight_2d, ord=2, axis=1)
        return self.lmbda * tf.reduce_sum(group_norms)

    def get_config(self):
        return {'lmbda': self.lmbda, 'n_features': self.n_features}

# -------------------------------
# LSTM-Based Granger Causality Analysis Function
# -------------------------------
def analyze_granger(df, target, features, timesteps, lmbda, units, epochs, batch_size):
    """
    Build and train an LSTM model with group Lasso regularization to determine feature importance
    for predicting the target variable. The function creates time-series sequences using the specified
    timesteps, trains the model, and computes the group norm (importance) for each feature.
    """
    # Extract predictor (X) and target (y) values from the DataFrame.
    X = df[features].values
    y = df[target].values

    # Create sequential data using a sliding window of length 'timesteps'.
    X_seq, y_seq = [], []
    for i in range(timesteps, len(X)):
        X_seq.append(X[i-timesteps:i, :])
        y_seq.append(y[i])
    X_seq = np.array(X_seq, dtype='float32')
    y_seq = np.array(y_seq, dtype='float32')

    # Split the data into training and validation sets while preserving time order.
    X_train, X_val, y_train, y_val = train_test_split(X_seq, y_seq, test_size=0.2, shuffle=False)

    # Build the LSTM model.
    input_layer = Input(shape=(timesteps, len(features)))
    # Add an LSTM layer with the custom Group Lasso regularizer applied to its kernel weights.
    lstm_layer = LSTM(units, kernel_regularizer=GroupLassoRegularizer(lmbda, len(features)))(input_layer)
    # Add a Dense output layer for regression (predicting a continuous target value).
    output = Dense(1)(lstm_layer)
    model = Model(inputs=input_layer, outputs=output)
    model.compile(optimizer='adam', loss='mse')

    # Set up callbacks for early stopping and learning rate reduction.
    callbacks = [
        EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True),
        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-7)
    ]

    # Train the model (verbose=0 suppresses output).
    model.fit(X_train, y_train, validation_data=(X_val, y_val),
              epochs=epochs, batch_size=batch_size, callbacks=callbacks, verbose=0)

    # Extract the LSTM kernel weights using get_weights().
    # For LSTM, the first element of get_weights() is the kernel with shape (input_dim, 4*units).
    lstm_weights = model.layers[1].get_weights()[0]
    # Reshape weights so that rows correspond to each feature (n_features, 4*units).
    feature_importance = np.linalg.norm(lstm_weights.reshape(len(features), -1), axis=1)

    # Return a dictionary mapping each feature to its computed importance (group norm).
    return dict(zip(features, feature_importance))

# -------------------------------
# Permutation Testing (Parallelized)
# -------------------------------
def parallel_permutation_test(df, target, features, timesteps, lmbda, units, epochs, batch_size, num_permutations, num_jobs):
    """
    Generate a null distribution for feature importance by shuffling the target variable and
    re-running the analysis multiple times in parallel.
    """
    def single_permutation(_):
        # Copy the DataFrame to shuffle the target without affecting the original.
        df_shuffled = df.copy()
        # Shuffle the target column while preserving the index (to maintain time alignment).
        df_shuffled[target] = df_shuffled[target].sample(frac=1).values
        # Compute feature importance for the shuffled data.
        return analyze_granger(df_shuffled, target, features, timesteps, lmbda, units, epochs, batch_size)

    # Run the permutation tests in parallel using joblib's Parallel.
    null_dists = Parallel(n_jobs=num_jobs)(delayed(single_permutation)(i) for i in range(num_permutations))
    # Organize the null distribution into a dictionary with one list per feature.
    return {f: [res[f] for res in null_dists] for f in features}

# -------------------------------
# Main Analysis Workflow
# -------------------------------
results = {}       # To store actual feature importance scores for each target
p_values_all = {}  # To store p-values for each predictor for each target

# Iterate over each asset (target) and treat the others as predictors.
for target in crypto_columns:
    print(f"Analyzing target: {target}")
    # Define predictors as all columns except the current target.
    features = [col for col in crypto_columns if col != target]

    # Compute true feature importance using the actual (non-shuffled) data.
    true_importance = analyze_granger(df_stationary, target, features, timesteps, lmbda, units, epochs, batch_size)

    # Generate a null distribution via permutation testing.
    null_distribution = parallel_permutation_test(df_stationary, target, features, timesteps, lmbda, units, epochs, batch_size, num_permutations, num_jobs)

    # Compute p-values using the standard procedure:
    # p-value = ((100 - percentile) / 100) * (number of features), capped at 1.0.
    pvals = {}
    for f in features:
        obs = true_importance[f]
        null = null_distribution[f]
        perc = percentileofscore(null, obs)  # This returns a value between 0 and 100.
        p_value = ((100 - perc) / 100) * len(features)
        p_value = min(p_value, 1.0)  # Ensure p-value does not exceed 1.0.
        pvals[f] = p_value

    results[target] = true_importance
    p_values_all[target] = pvals

# -------------------------------
# Convert p-values into a DataFrame for visualization
# -------------------------------
# Create a matrix with targets as rows and predictors as columns.
p_value_df = pd.DataFrame(index=crypto_columns, columns=crypto_columns)
for target in crypto_columns:
    for feature in crypto_columns:
        if target == feature:
            p_value_df.loc[target, feature] = np.nan  # Self-causality is not tested.
        else:
            p_value_df.loc[target, feature] = p_values_all[target].get(feature, np.nan)

# Print the resulting p-value matrix.
print("\nP-values for Granger Causality (Permutation Testing):")
print(p_value_df)

# -------------------------------
# Visualization of Results
# -------------------------------
# Plot a heatmap of the full p-value matrix.
plt.figure(figsize=(12, 10))
sns.heatmap(p_value_df.astype(float), annot=True, fmt=".3f", cmap='rocket_r', linewidths=0.5, square=True,
            cbar_kws={'label': 'Adjusted p-value'})
plt.title(f'LSTM-Based Granger Causality (Timesteps = {timesteps})', fontsize=16, fontweight='bold')
plt.xlabel('Predictor Variables', fontsize=14)
plt.ylabel('Target Variables', fontsize=14)
plt.tight_layout()
plt.show()

# -------------------------------
# Run the main analysis loop and store null distributions for each target
# -------------------------------
results = {}                # Stores true feature importance for each target.
p_values_all = {}           # Stores p-values for each predictor for each target.
null_distributions_all = {} # New: To store null distributions per target.

for target in crypto_columns:
    print(f"Analyzing target: {target}")
    # Define predictors as all columns except the current target.
    features = [col for col in crypto_columns if col != target]

    # Compute true feature importance using the actual (non-shuffled) data.
    true_importance = analyze_granger(df_stationary, target, features, timesteps, lmbda, units, epochs, batch_size)

    # Generate a null distribution via permutation testing.
    null_distribution = parallel_permutation_test(df_stationary, target, features, timesteps, lmbda, units, epochs, batch_size, num_permutations, num_jobs)

    # Store the null distribution for this target.
    null_distributions_all[target] = null_distribution

    # Compute p-values using the standard procedure.
    pvals = {}
    for f in features:
        obs = true_importance[f]
        null = null_distribution[f]
        perc = percentileofscore(null, obs)  # Value between 0 and 100.
        p_value = ((100 - perc) / 100) * len(features)
        pvals[f] = min(p_value, 1.0)

    results[target] = true_importance
    p_values_all[target] = pvals

# -------------------------------
# Visualization: Plot the null distribution for each predictor for every target
# -------------------------------
for target in crypto_columns:
    # Define the predictors (explanatory variables) for the current target.
    features = [col for col in crypto_columns if col != target]

    for feature in features:
        null_vals = null_distributions_all[target][feature]  # Norm values from each permutation trial.
        true_val = results[target][feature]  # True importance from the actual data.

        plt.figure(figsize=(8, 6))
        plt.hist(null_vals, bins=10, alpha=0.7, label='Null Distribution')
        plt.axvline(x=true_val, color='red', linestyle='--', label='True Importance')
        plt.title(f'Null Distribution for {feature}\n(Target: {target})')
        plt.xlabel('Norm Value')
        plt.ylabel('Frequency')
        plt.legend()
        plt.tight_layout()
        plt.show()

# Install necessary packages in Colab
!pip install tensorflow pandas numpy openpyxl scikit-learn seaborn matplotlib

import tensorflow as tf
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.models import Model
from tensorflow.keras.regularizers import Regularizer
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# Set visualization style (optional)
plt.rcParams['font.family'] = 'Times New Roman'
plt.rcParams['font.size'] = 14
sns.set_style("whitegrid")
sns.set_context("talk", font_scale=1)

# -------------------------------
# Custom Group Lasso Regularizer for MLP
# -------------------------------
class GroupLassoMLPRegularizer(Regularizer):
    def __init__(self, lmbda, n_features, timesteps):
        self.lmbda = lmbda
        self.n_features = n_features
        self.timesteps = timesteps

    def __call__(self, weight_matrix):
        # Reshape weight matrix: assume input shape is (timesteps * n_features,)
        # and weight_matrix shape is (timesteps * n_features, units)
        new_shape = (self.n_features, self.timesteps, -1)
        weight_matrix_reshaped = tf.reshape(weight_matrix, new_shape)
        # Compute group norm for each feature (across all timesteps and units)
        group_norms = tf.norm(weight_matrix_reshaped, ord='euclidean', axis=[1, 2])
        return self.lmbda * tf.reduce_sum(group_norms)

    def get_config(self):
        return {'lmbda': self.lmbda, 'n_features': self.n_features, 'timesteps': self.timesteps}

# -------------------------------
# Data Loading and Preprocessing
# -------------------------------
# Load data from the Excel file (update path as necessary)
df_crypto = pd.read_excel('/content/combined_data.xlsx', sheet_name='AllData')

# Select and rename columns for BTC, ETH, ADA, XRP, BCH, XAU, and CrudeOil
# (Note: USDX_Close is removed based on updated data.)
df_closing = df_crypto[['Date', 'BTC_Close', 'ETH_Close', 'ADA_Close',
                         'XRP_Close', 'BCH_Close', 'XAU_Close', 'CrudeOil_Close']]
df_closing.columns = ['Date', 'BTC_Close', 'ETH_Close', 'ADA_Close',
                        'XRP_Close', 'BCH_Close', 'XAU_Close', 'CrudeOil_Close']
df_closing['Date'] = pd.to_datetime(df_closing['Date'])
df_closing.dropna(inplace=True)
df_closing.sort_values('Date', inplace=True)

# Define the asset columns
cols = ['BTC_Close', 'ETH_Close', 'ADA_Close', 'XRP_Close', 'BCH_Close', 'XAU_Close', 'CrudeOil_Close']

# Ensure stationarity by taking the first difference
df_stationary = df_closing.copy()
for col in cols:
    df_stationary[col] = df_stationary[col].diff()
df_stationary.dropna(inplace=True)

# Normalize features (improves NN convergence)
scaler = StandardScaler()
df_stationary[cols] = scaler.fit_transform(df_stationary[cols])

# -------------------------------
# Neural Network Granger Causality Analysis Function (MLP)
# -------------------------------
def analyze_granger_mlp(df, target, features, timesteps, units, lmbda, epochs=30, batch_size=16):
    X = df[features].values
    y = df[target].values

    # Create sequential (time-lagged) data for each sample
    # For MLP, we flatten the timesteps so that each sample is a vector of shape (timesteps * n_features,)
    X_seq, y_seq = [], []
    for i in range(timesteps, len(X)):
        # Take timesteps rows and flatten them: shape becomes (timesteps * len(features),)
        X_seq.append(X[i-timesteps:i].flatten())
        y_seq.append(y[i])
    X_seq = np.array(X_seq, dtype='float32')
    y_seq = np.array(y_seq, dtype='float32')

    # Split data into training and validation sets (preserving temporal order)
    X_train, X_val, y_train, y_val = train_test_split(X_seq, y_seq, test_size=0.2, shuffle=False)

    # Build the MLP model:
    # Input shape is (timesteps * number of features,)
    input_seq = Input(shape=(timesteps * len(features),))
    # Dense layer with ReLU activation and custom Group Lasso regularizer
    dense_out = Dense(units, activation='relu', kernel_initializer='glorot_uniform',
                      kernel_regularizer=GroupLassoMLPRegularizer(lmbda, n_features=len(features), timesteps=timesteps))(input_seq)
    output = Dense(1)(dense_out)
    model = Model(inputs=input_seq, outputs=output)
    model.compile(optimizer='adam', loss='mse')

    # Callbacks: early stopping and reducing LR if needed
    callbacks = [
        EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),
        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)
    ]
    model.fit(X_train, y_train, validation_data=(X_val, y_val),
              epochs=epochs, batch_size=batch_size, callbacks=callbacks, verbose=1)

    # Extract kernel weights from the Dense layer and reshape to (n_features, timesteps, units)
    kernel_weights = model.layers[1].kernel.numpy().reshape(len(features), timesteps, units)
    group_norms = np.linalg.norm(kernel_weights, axis=(1, 2))

    return dict(zip(features, group_norms))

# -------------------------------
# Run Iterative Granger Causality Analysis with MLP
# -------------------------------
results_mlp = {}
for target in cols:
    feature_cols = [col for col in cols if col != target]
    print(f"\nAnalyzing Granger causality for {target} using MLP ...")
    norms = analyze_granger_mlp(df_stationary, target, feature_cols, timesteps=10, units=20, lmbda=0.01, epochs=30, batch_size=16)
    results_mlp[target] = norms

# Convert the results (a dictionary of dictionaries) into a square DataFrame with NaN on the diagonal.
desired_order = ['BTC_Close', 'ETH_Close', 'ADA_Close', 'XRP_Close', 'BCH_Close', 'XAU_Close', 'CrudeOil_Close']
group_norm_matrix_mlp = pd.DataFrame(index=desired_order, columns=desired_order)
for target in desired_order:
    for feature in desired_order:
        if target == feature:
            group_norm_matrix_mlp.loc[target, feature] = np.nan
        else:
            group_norm_matrix_mlp.loc[target, feature] = results_mlp[target].get(feature, np.nan)

print("MLP-based Granger causality (group norm) analysis results (rearranged):")
print(group_norm_matrix_mlp)

# -------------------------------
# (Optional) Visualization: Heatmap for MLP-based Granger Causality Results
# -------------------------------
def clean_labels(df):
    df_clean = df.copy()
    df_clean.index = df_clean.index.str.replace('_Close', '')
    df_clean.columns = df_clean.columns.str.replace('_Close', '')
    return df_clean

df_mlp_display = clean_labels(group_norm_matrix_mlp).astype(float)

plt.figure(figsize=(10, 8))
sns.heatmap(df_mlp_display, annot=True, fmt=".2f", cmap='coolwarm', linewidths=0.5, square=True, cbar_kws={'shrink': 0.8})
plt.title('MLP-based Granger Causality (Group Norms) Heatmap', fontsize=16, fontweight='bold')
plt.xlabel('Features (Predictors)', fontsize=14)
plt.ylabel('Target Variables', fontsize=14)
plt.tight_layout()
plt.show()

import pandas as pd
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
import seaborn as sns
from joblib import Parallel, delayed
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.models import Model
from tensorflow.keras.regularizers import Regularizer
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from scipy.stats import percentileofscore

# ---------------------------------
# 🚀 USER-DEFINED PARAMETERS
# ---------------------------------
HIGH_COMPUTATION_MODE = False  # True for rigorous analysis
num_permutations = 30 if not HIGH_COMPUTATION_MODE else 100
num_jobs = 4 if not HIGH_COMPUTATION_MODE else -1  # CPU cores
units = 10 if not HIGH_COMPUTATION_MODE else 20
epochs = 20 if not HIGH_COMPUTATION_MODE else 50
batch_size = 32 if not HIGH_COMPUTATION_MODE else 16
timesteps = 3  # ✅ CRITICAL FIX: Number of time lags for Granger analysis

# ---------------------------------
# 🚀 Load & Preprocess Data
# ---------------------------------
df_crypto = pd.read_excel('/content/combined_data.xlsx', sheet_name='AllData')
df_closing = df_crypto[['Date', 'BTC_Close', 'ETH_Close', 'ADA_Close',
                        'XRP_Close', 'BCH_Close', 'XAU_Close', 'CrudeOil_Close']].copy()

df_closing['Date'] = pd.to_datetime(df_closing['Date'])
df_closing.dropna(inplace=True)
df_closing.sort_values('Date', inplace=True)

cols = ['BTC_Close', 'ETH_Close', 'ADA_Close', 'XRP_Close', 'BCH_Close', 'XAU_Close', 'CrudeOil_Close']

# Stationarity and Scaling
df_stationary = df_closing.copy()
for col in cols:
    df_stationary[col] = df_stationary[col].diff()
df_stationary.dropna(inplace=True)

scaler = StandardScaler()
df_stationary[cols] = scaler.fit_transform(df_stationary[cols])

# ---------------------------------
# 🚀 Enhanced Group Lasso Regularizer
# ---------------------------------
class GroupLassoMLPRegularizer(Regularizer):
    def __init__(self, lmbda, n_features, timesteps):
        self.lmbda = lmbda
        self.n_features = n_features
        self.timesteps = timesteps

    def __call__(self, weight_matrix):
        # Reshape weights: (features, timesteps, units)
        new_shape = (self.n_features, self.timesteps, -1)
        weight_reshaped = tf.reshape(weight_matrix, new_shape)
        group_norms = tf.norm(weight_reshaped, ord=2, axis=(1, 2))  # L2 norm per feature group
        return self.lmbda * tf.reduce_sum(group_norms)

    def get_config(self):
        return {'lmbda': self.lmbda, 'n_features': self.n_features, 'timesteps': self.timesteps}

# ---------------------------------
# 🚀 Optimized MLP Granger Analysis
# ---------------------------------
def analyze_granger_mlp(df, target, features, timesteps, lmbda=0.01):
    X = df[features].values
    y = df[target].values

    # Create sequential data with lags
    X_seq, y_seq = [], []
    for i in range(timesteps, len(X)):
        X_seq.append(X[i-timesteps:i].flatten())  # Flatten lags into 1D
        y_seq.append(y[i])
    X_seq = np.array(X_seq, dtype='float32')
    y_seq = np.array(y_seq, dtype='float32')

    # Train/validation split (no shuffle for time series)
    X_train, X_val, y_train, y_val = train_test_split(X_seq, y_seq, test_size=0.2, shuffle=False)

    # Build MLP with Group Lasso
    input_layer = Input(shape=(timesteps*len(features),))
    x = Dense(units, activation='relu',
              kernel_regularizer=GroupLassoMLPRegularizer(lmbda, len(features), timesteps))(input_layer)
    output = Dense(1)(x)
    model = Model(inputs=input_layer, outputs=output)
    model.compile(optimizer='adam', loss='mse')

    # Train with early stopping
    history = model.fit(X_train, y_train,
                        validation_data=(X_val, y_val),
                        epochs=epochs,
                        batch_size=batch_size,
                        callbacks=[
                            EarlyStopping(patience=5, restore_best_weights=True),
                            ReduceLROnPlateau(factor=0.5, patience=3)
                        ],
                        verbose=0)

    # Extract feature importance from first layer weights
    weights = model.layers[1].kernel.numpy()
    weights_3d = weights.reshape(len(features), timesteps, -1)
    feature_importance = np.linalg.norm(weights_3d, axis=(1, 2))  # L2 norm per feature

    return dict(zip(features, feature_importance))

# ---------------------------------
# 🚀 Parallel Permutation Testing
# ---------------------------------
def parallel_permutation_test(df, target, features, timesteps):
    def single_permutation(_):
        df_shuffled = df.copy()
        # Shuffle target while preserving feature structure
        df_shuffled[target] = df_shuffled[target].sample(frac=1).values  # ✅ Proper shuffling
        return analyze_granger_mlp(df_shuffled, target, features, timesteps)

    null_dists = Parallel(n_jobs=num_jobs)(
        delayed(single_permutation)(i) for i in range(num_permutations)

    # Aggregate null distributions
    null_distribution = {f: [] for f in features}
    for result in null_dists:
        for f in features:
            null_distribution[f].append(result[f])
    return null_distribution

# ---------------------------------
# 🚀 Main Analysis Workflow
# ---------------------------------
results_mlp = {}
p_values_mlp = {}

for target in cols:
    print(f"\n🔍 Analyzing target: {target}")
    features = [col for col in cols if col != target]

    # Real importance
    true_importance = analyze_granger_mlp(df_stationary, target, features, timesteps)

    # Null distribution
    null_dist = parallel_permutation_test(df_stationary, target, features, timesteps)

    # Calculate p-values
    pvals = {}
    for f in features:
        null_samples = null_dist[f]
        true_value = true_importance[f]
        pvals[f] = 1 - (percentileofscore(null_samples, true_value) / 100)

    results_mlp[target] = true_importance
    p_values_mlp[target] = pvals

# ---------------------------------
# 🚀 Enhanced Visualization
# ---------------------------------
p_value_df = pd.DataFrame(p_values_mlp).T
p_value_df.columns = cols
p_value_df.index = cols

plt.figure(figsize=(12, 10))
mask = np.triu(np.ones_like(p_value_df, dtype=bool))  # Show lower triangle only
sns.heatmap(p_value_df, mask=mask, annot=True, fmt=".3f",
            cmap='viridis', linewidths=0.5, vmin=0, vmax=0.15,
            cbar_kws={'label': 'p-value'})
plt.title('MLP-Based Granger Causality Network (Significance Level: p < 0.1)',
          fontsize=14, pad=20)
plt.xlabel('Potential Causes', fontsize=12)
plt.ylabel('Effects', fontsize=12)
plt.tight_layout()
plt.show()

# Install necessary packages in Colab
!pip install tensorflow pandas numpy openpyxl scikit-learn seaborn matplotlib networkx

import tensorflow as tf
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import networkx as nx
from tensorflow.keras.layers import Input, LSTM, Dense, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.regularizers import Regularizer
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# Set visualization style for publication-quality graphics
plt.rcParams['font.family'] = 'Times New Roman'
plt.rcParams['font.size'] = 14
sns.set_style("whitegrid")
sns.set_context("talk", font_scale=1)

# -------------------------------
# Custom Group Lasso Regularizer for LSTM
# -------------------------------
class GroupLassoRegularizer(Regularizer):
    def __init__(self, lmbda):
        self.lmbda = lmbda
    def __call__(self, weight_matrix):
        group_norms = tf.norm(weight_matrix, axis=1)
        return self.lmbda * tf.reduce_sum(group_norms)
    def get_config(self):
        return {'lmbda': self.lmbda}

# -------------------------------
# Custom Group Lasso Regularizer for MLP
# -------------------------------
class GroupLassoMLPRegularizer(Regularizer):
    def __init__(self, lmbda, n_features, timesteps):
        self.lmbda = lmbda
        self.n_features = n_features
        self.timesteps = timesteps
    def __call__(self, weight_matrix):
        # Reshape weight matrix to (n_features, timesteps, units)
        new_shape = (self.n_features, self.timesteps, -1)
        weight_matrix_reshaped = tf.reshape(weight_matrix, new_shape)
        group_norms = tf.norm(weight_matrix_reshaped, ord='euclidean', axis=[1, 2])
        return self.lmbda * tf.reduce_sum(group_norms)
    def get_config(self):
        return {'lmbda': self.lmbda, 'n_features': self.n_features, 'timesteps': self.timesteps}

# -------------------------------
# Data Loading and Preprocessing
# -------------------------------
# Load data from the Excel file (update path if necessary)
df_crypto = pd.read_excel('/content/combined_data.xlsx', sheet_name='AllData')

# Select and rename columns for BTC, ETH, ADA, XRP, BCH, XAU, and CrudeOil.
# (Note: 'USDX_Close' is removed as it is not present in the updated data.)
df_closing = df_crypto[['Date', 'BTC_Close', 'ETH_Close', 'ADA_Close',
                         'XRP_Close', 'BCH_Close', 'XAU_Close', 'CrudeOil_Close']]
df_closing.columns = ['Date', 'BTC_Close', 'ETH_Close', 'ADA_Close',
                        'XRP_Close', 'BCH_Close', 'XAU_Close', 'CrudeOil_Close']
df_closing['Date'] = pd.to_datetime(df_closing['Date'])
df_closing.dropna(inplace=True)
df_closing.sort_values('Date', inplace=True)

# Define asset columns
crypto_columns = ['BTC_Close', 'ETH_Close', 'ADA_Close', 'XRP_Close', 'BCH_Close', 'XAU_Close', 'CrudeOil_Close']

# Ensure stationarity: take the first difference
df_stationary = df_closing.copy()
for col in crypto_columns:
    df_stationary[col] = df_stationary[col].diff()
df_stationary.dropna(inplace=True)

# Normalize features for improved NN convergence
scaler = StandardScaler()
df_stationary[crypto_columns] = scaler.fit_transform(df_stationary[crypto_columns])

# -------------------------------
# Neural Network Granger Causality Analysis Functions
# -------------------------------

# LSTM-based analysis
def analyze_granger_lstm(df, target, features, timesteps=10, units=20, lmbda=0.01, epochs=50, batch_size=16):
    X = df[features].values
    y = df[target].values
    X_seq, y_seq = [], []
    for i in range(timesteps, len(X)):
        X_seq.append(X[i-timesteps:i, :])
        y_seq.append(y[i])
    X_seq = np.array(X_seq, dtype='float32')
    y_seq = np.array(y_seq, dtype='float32')
    X_train, X_val, y_train, y_val = train_test_split(X_seq, y_seq, test_size=0.2, shuffle=False)
    input_seq = Input(shape=(timesteps, len(features)))
    lstm_out = LSTM(units, kernel_regularizer=GroupLassoRegularizer(lmbda), return_sequences=False)(input_seq)
    dropout_out = Dropout(0.2)(lstm_out)
    output = Dense(1)(dropout_out)
    model = Model(inputs=input_seq, outputs=output)
    model.compile(optimizer='adam', loss='mse')
    callbacks = [
        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),
        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)
    ]
    model.fit(X_train, y_train, validation_data=(X_val, y_val),
              epochs=epochs, batch_size=batch_size, callbacks=callbacks, verbose=0)
    kernel_weights = model.layers[1].cell.kernel.numpy()
    group_norms = np.linalg.norm(kernel_weights, axis=1)
    return dict(zip(features, group_norms))

# MLP-based analysis
def analyze_granger_mlp(df, target, features, timesteps, units, lmbda, epochs=30, batch_size=16):
    X = df[features].values
    y = df[target].values
    X_seq, y_seq = [], []
    for i in range(timesteps, len(X)):
        X_seq.append(X[i-timesteps:i].flatten())
        y_seq.append(y[i])
    X_seq = np.array(X_seq, dtype='float32')
    y_seq = np.array(y_seq, dtype='float32')
    X_train, X_val, y_train, y_val = train_test_split(X_seq, y_seq, test_size=0.2, shuffle=False)
    input_seq = Input(shape=(timesteps * len(features),))
    dense_out = Dense(units, activation='relu', kernel_initializer='glorot_uniform',
                      kernel_regularizer=GroupLassoMLPRegularizer(lmbda, n_features=len(features), timesteps=timesteps))(input_seq)
    output = Dense(1)(dense_out)
    model = Model(inputs=input_seq, outputs=output)
    model.compile(optimizer='adam', loss='mse')
    callbacks = [
        EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),
        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)
    ]
    model.fit(X_train, y_train, validation_data=(X_val, y_val),
              epochs=epochs, batch_size=batch_size, callbacks=callbacks, verbose=0)
    kernel_weights = model.layers[1].kernel.numpy().reshape(len(features), timesteps, units)
    group_norms = np.linalg.norm(kernel_weights, axis=(1, 2))
    return dict(zip(features, group_norms))

# -------------------------------
# Run Iterative Granger Causality Analysis for Both Models
# -------------------------------
results_lstm = {}
results_mlp = {}
for target in crypto_columns:
    features = [col for col in crypto_columns if col != target]
    print(f"\nAnalyzing target: {target} using LSTM...")
    norms_lstm = analyze_granger_lstm(df_stationary, target, features, timesteps=10, units=20, lmbda=0.01, epochs=50, batch_size=16)
    results_lstm[target] = norms_lstm
    print(f"Analyzing target: {target} using MLP...")
    norms_mlp = analyze_granger_mlp(df_stationary, target, features, timesteps=10, units=20, lmbda=0.01, epochs=30, batch_size=16)
    results_mlp[target] = norms_mlp

# Function to build square DataFrame from results dictionary
def build_group_norm_matrix(results, order):
    mat = pd.DataFrame(index=order, columns=order)
    for target in order:
        for feature in order:
            if target == feature:
                mat.loc[target, feature] = np.nan
            else:
                mat.loc[target, feature] = results[target].get(feature, np.nan)
    return mat

desired_order = ['BTC_Close', 'ETH_Close', 'ADA_Close', 'XRP_Close', 'BCH_Close', 'XAU_Close', 'CrudeOil_Close']
group_norm_matrix_lstm = build_group_norm_matrix(results_lstm, desired_order)
group_norm_matrix_mlp = build_group_norm_matrix(results_mlp, desired_order)

print("LSTM-based Granger causality (group norm) analysis results (rearranged):")
print(group_norm_matrix_lstm)
print("MLP-based Granger causality (group norm) analysis results (rearranged):")
print(group_norm_matrix_mlp)

# -------------------------------
# Compute Pearson Correlation Matrix (for reference)
# -------------------------------
corr_matrix = df_stationary[crypto_columns].corr()
corr_matrix = corr_matrix.reindex(index=desired_order, columns=desired_order)
print("Reordered Pearson correlation matrix:")
print(corr_matrix)

# -------------------------------
# Network Diagram Construction Function
# -------------------------------
def build_network_diagram(group_norm_matrix, threshold, title):
    G = nx.DiGraph()
    # Add nodes with cleaned labels
    for node in group_norm_matrix.index:
        G.add_node(node.replace('_Close',''))
    # Add edges if the value is above the threshold (edge from predictor to target)
    for target in group_norm_matrix.index:
        for predictor in group_norm_matrix.columns:
            if target != predictor:
                val = group_norm_matrix.loc[target, predictor]
                if pd.notna(val) and val >= threshold:
                    G.add_edge(predictor.replace('_Close',''), target.replace('_Close',''), weight=val)
    pos = nx.circular_layout(G)
    plt.figure(figsize=(10,8))
    nx.draw_networkx_nodes(G, pos, node_size=500, node_color='lightblue')
    nx.draw_networkx_labels(G, pos, font_family='Times New Roman', font_size=12)
    edges = G.edges(data=True)
    edge_widths = [data['weight']*2 for u, v, data in edges]  # scale for visualization
    nx.draw_networkx_edges(G, pos, arrowstyle='->', arrowsize=20, width=edge_widths, edge_color='gray')
    plt.title(title, fontsize=16, fontweight='bold')
    plt.axis('off')
    plt.show()

# Function to compute threshold based on the 60th percentile of non-NaN values in the matrix
def compute_threshold(mat):
    vals = mat.values.flatten()
    vals = vals[~pd.isna(vals)]
    return np.percentile(vals, 60)

threshold_lstm = compute_threshold(group_norm_matrix_lstm)
threshold_mlp = compute_threshold(group_norm_matrix_mlp)
print("LSTM threshold:", threshold_lstm)
print("MLP threshold:", threshold_mlp)

# Build and display network diagrams for LSTM and MLP-based results
build_network_diagram(group_norm_matrix_lstm, threshold_lstm, "LSTM-based Neural Granger Causality Network Diagram")
build_network_diagram(group_norm_matrix_mlp, threshold_mlp, "MLP-based Neural Granger Causality Network Diagram")

